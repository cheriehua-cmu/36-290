---
title: "Lab_11R"
author: "36-290 -- Statistical Research Methodology"
date: "Week 11 Thursday -- Fall 2020"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

Name: Cherie Hua

Andrew ID: cxhua

This lab is to be begun in class, but may be finished outside of class at any time prior to Friday, November 13<sup>th</sup> at 6:00 PM. You must commit both the edited `Rmd` file and the "knitted" `html` file for this lab to your `GitHub` repo. (You can verbally discuss aspects of the labs with your classmates, but please do not share code, etc.)

---

# Preliminaries

## Goal

To practice both kernel density estimation and kernel regression.

## Data

Below we read in the `EMLINE_MASS` dataset, in which the strengths of 10 emission lines are recorded for each of 21,046 galaxies, along with the galaxy masses. Relating the 10 emission line strengths to the galaxy masses was a Fall 2018 semester project.
```{r}
rm(list=ls())
file.path = "https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/EMLINE_MASS/emission_line_mass.Rdata"
load(url(file.path))
rm(file.path)
x = predictors$H_ALPHA
x.tmp = log10(x[x>0])
y     = responses[x>0,1]
x     = x.tmp
df    = data.frame(x,y)
```
Today we are simply playing around with kernel density estimation and kernel regression, so all we are going to keep is the values for the strongest emission line, the so-called "H$\alpha$" line at 656 nanometers (which we will call $x$), and the masses (which we will call $y$). We also filter the data so as to keep only positive emission line strengths, so that we can implement a logarithmic transformation for $x$.

# Questions

## Question 1

Do some EDA. First, use `ggplot2` to create histograms of both $x$ and $y$, and then use it to make a scatter plot of $x$ and $y$.
```{r}
library(ggplot2)
ggplot(df, aes(x = x)) + geom_histogram(bins = 100, color = "chartreuse3")
ggplot(df, aes(x = y)) + geom_histogram(bins = 100, color = "chartreuse1")
ggplot(df, aes(x = x, y = y)) + geom_point(color = "tomato", alpha = 0.1)
```

## Question 2

Create a density estimate for $x$ using the `density()` function and the default bandwidth. Print the default bandwidth. Then overlay the density estimate on top of a density histogram. One creates a density histogram by adding an extra argument to `geom_histogram()`: `aes(y=..density..)`. One can then overlay the density estimate using an additional call to `geom_line()`.
```{r}
dens <- density(x)
dens$bw

newdf <- data.frame(dens$x, dens$y)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..), position = "identity", fill = "blue") + geom_line(newdf, mapping = aes(x = dens$x, y = dens$y), color = "red")
```

## Question 3

Using the formula for the Silverman rule that is given in the notes, compute the default bandwidth by hand. Do you get the same value as returned by `density()`? (If you don't...you coded the formula incorrectly.)
```{r}
0.9 * min(sd(x), IQR(x)/1.34) * length(x)^(-1/5)
```
```
Yes, I got the same value: 0.0954.
```

## Question 4

Repeat Q2, but use the unbiased cross-validation estimator, whose use is specified in the notes. Again, print the bandwidth and make the same density estimate overlaid onto histogram plot as in Q2. Stare hard at the two plots, the one here and the one in Q2: can you see any differences in the density estimates?
```{r}
dens <- density(x, bw = "ucv")
dens$bw

newdf <- data.frame(dens$x, dens$y)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..), position = "identity", fill = "blue") + geom_line(newdf, mapping = aes(x = dens$x, y = dens$y), color = "red")
```
```
I can't tell if there are any differences in the density estimates. They look the same.
```

## Question 5

Density estimates tend to work fine with unbounded data, but can exhibit boundary bias if the data values are bounded on either or both sides. Repeat Q4, except run the code for only $x$ values between 0 and 1, and set the bandwidth manually to 0.1. What do you observe?
```{r}
dens <- density(which(x >= 0, x <= 1), bw = 0.1)

newdf <- data.frame(dens$x, dens$y)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..), position = "identity", fill = "blue") + geom_line(newdf, mapping = aes(x = dens$x, y = dens$y), color = "red")
```
```
It looks like the density estimates are varying around 0.0075, which is higher than in previous questions.
```

## Question 6

Pick 20 points at random from the initial, unbounded $x$ sample. Perform density estimates with "gaussian", "triangular", and "epanechnikov" kernels. Use `ggplot()` to draw the three density estimates (without the histogram). Do you see any significant differences in the estimates? Change the number of randomly sampled points to 500 and redo the plot...are there still any discernible differences?
```{r}
d <- sample(x, 500)
dens <- vector(length = 3)
kernels <- c("gaussian", "triangular", "epanechnikov")
for (i in length(kernels)) {
  de <- density(d, kernel = kernels[i])
  dens[i] <- data.frame(de$x, de$y)
}
```
```
Not sure what's wrong with my code, but my ggplot won't work.
```

## Question 7

Estimate galaxy mass from emission-line strength using the Nadaraya-Watson kernel estimator.

In the normal model learning paradigm, you split the data and learn the model using the training data, then apply the model to predict response values for the test data. You then compute the MSE.

For Nadaraya-Watson, the way this would play out is that we would split the data, then perform, e.g., cross-validation on the *training* set to determine the optimal value of $h$. We would then apply this value of $h$ when working with the test data, and when computing the MSE.

Here, we are going to keep things simpler: do not split the data, and compute a plug-in value of $h$ using one of the `bandwidth` functions in the base `stats` package. (Type, e.g., `?bw.nrd0` at the prompt in the Console pane.) Estimate $\hat{y}$ for all the data using a Gaussian kernel, then plot the predicted response vs. the observed response. (Note that this is a little tricky! First, you have to specify `x.points=x` in the call to `ksmooth()`,
so that the model is actually evaluated at the input points $x$ rather than along a default grid. Then you have to compare `out$y` versus `y[order(x)]` in the diagnostic plot, because `ksmooth()` sorts the $x$ values in ascending
order. This is all a bit painful to figure out. Your final diagnostic plot won't look that great...but that's OK, because we've really simplified the regression here [only one predictor variable, not 10].)
```{r}
h <- bw.nrd0(x)
out <- density(x, kernel = "gaussian")
plot(ksmooth(x.points = x, x = out$y, y=y[order(x)]))
```

