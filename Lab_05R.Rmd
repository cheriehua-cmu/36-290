---
title: "Lab_05R"
author: "36-290 -- Statistical Research Methodology"
date: "Week 5 Thursday -- Fall 2020"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---
Name: Cherie Hua

Andrew ID: cxhua

This lab is to be begun in class, but may be finished outside of class at any time prior to Friday, October 2<sup>nd</sup> at 6:00 PM. You must commit both the edited `Rmd` file and the "knitted" `html` file for this lab to your `GitHub` repo. (You can verbally discuss aspects of the labs with your classmates, but please do not share code, etc.)

---

# Preliminaries

## Goal

Today you will create a logistic regression model for classifying stars vs. quasars and you will assess your performance by computing a test-set misclassification rate.

## Data

We'll begin by importing data on stars and quasars:
```{r}
rm(list=ls())
file.path = "https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/STAR_QUASAR/Star_Quasar.Rdata"
load(url(file.path))
rm(file.path)
objects()
library(GGally)
```

If everything loaded correctly, you should see one variables in your global environment: `df`. This data frame has 8 measurements each for 500 quasars (faraway active galactic nuclei that look like stars) and for 500 stars. The first five columns are u-, g-, r-, i-, and z-band magnitudes, the sixth column is redshift (high for quasars, approximately zero for stars), the seventh column is redshift error, and the eighth column is a factor variable that denotes the class (`QSO` or `STAR`).

The goal is to see if you can correctly classify each object. We will set up a predictor data frame with four colors and a magnitude, and a response vector that is a factor variable with two levels ("QSO" and "STAR"). (Including redshift as a predictor would be cheating: the redshift is how we know for sure whether the objects are quasars or stars in the first place!)
```{r}
col.ug = df$u.mag - df$g.mag
col.gr = df$g.mag - df$r.mag
col.ri = df$r.mag - df$i.mag
col.iz = df$i.mag - df$z.mag
mag.r  = df$r.mag
predictors = data.frame(col.ug,col.gr,col.ri,col.iz,mag.r)
response   = df$class
```

# Questions

## Question 1

Split the data into training and test sets. Then use `ggpairs()` to display the (full) predictor space, while using the argument `mapping=aes(color=response)` to use separate colors for quasars and for stars. Based on what you see, do you expect a clean separation between quasars and stars? (In other words, do you expect a low misclassification rate?)
```{r}
#split into training and testing
set.seed(389121)
dt = sort(sample(nrow(df), nrow(df)*.7))
train<-df[dt,]
test<-df[-dt,]

ggpairs(predictors, mapping=aes(color=response))
```
```
Based on the results, the colors seem mostly on top of each other, so I expect a high misclassification rate.
```

## Question 2

Using code on pages 156-158 of ISLR, carry out a logistic regression analysis of the star-quasar data, and display both the misclassification rate and a table of predictions versus test-set responses (i.e., display the confusion matrix). (Note: it may help you to use the `contrasts()` function to determine the mapping from the factor levels to actual numbers. See the top of page 158.) Challenges: can you create a vector of predicted factors in one line using the `ifelse()` function (which is *not* what ISLR does), and can use compute the misclassification rate using just one logical comparison?
```{r}

```
```
Yes, >.5 goes to class 1! Just be mindful of what R says is class 0 and 1 (like Q2 says)
For the written answer for part 2, do we only need to report the MCR?
Yes. Additionally, you can note to yourselves whether or not you would have expected that value given the appearances of the pairs plots.
Yeah, this is kind of a bug in the lab: you should just have to show the number in the code chunk without necessarily writing it laterâ€¦
```

## Question 3

Compute the sensitivity and specificity of logistic regression using definitions on the wikipedia page that we link to in the logistic regression notes. There can be some ambiguity regarding tables: assume that predicting that a QSO is a QSO is a "true positive" here, as opposed to predicting a star is a star.

Don't hard-code numbers! If you saved your confusion matrix above to the variable `tab`, then, e.g.,
```
TP = tab[1,1]
FP = tab[2,1]
```
etc. Map your table to `TP`, `FP`, `TN`, and `FN`, and use these to compute sensitivity and specificity, and then define each in words. In a perfect world, what would the sum of sensitivity and specificity be?
```{r}
# FILL ME IN
```
```
FILL ME IN
```

## Question 4

An astronomer might be more interested to know what proportion of objects that are predicted to be quasars actually are quasars. Compute this quantity and determine from the confusion matrix wikipedia page what this quantity is called.
```{r}
# FILL ME IN
```
```
FILL ME IN
```

## Question 5

While we didn't discuss this explicitly in the notes, we can attempt to visualize the distributions of the predicted binomial probabilities $\hat{p}$ versus class. Do that below. I'm going to leave it as ambiguous about how exactly you might do this.
```{r}
# FILL ME IN
```

## Question 6

You should be sufficiently comfortable with setting up basic analyses that you are going to do something different here: you are going to perform an analysis using a method not described in class. Linear discriminant analysis basically assumes that the predictors for the `QSO` class and for the `STAR` class are each sampled from a multivariate (specifically $p$-dimensional) normal distribution; the means are different for each class, but the "widths" of the distributions (as encoded in a covariance matrix) are the same. For each test datum, you can determine the estimated probability density for quasars, and the estimated probability density for stars; if the former is larger, we predict the datum is a quasar, and if the latter is larger, we predict the datum is a star. Those details aside, go to pages 160 and 161 of ISLR and implement an LDA analysis. Compute the misclassification rate and display the confusion matrix. Does LDA do better than logistic regression? Does it do worse?
```{r}
# FILL ME IN
```
```
FILL ME IN
```
