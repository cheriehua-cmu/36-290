---
title: "Lab_07R"
author: "36-290 -- Statistical Research Methodology"
date: "Week 7 Thursday -- Fall 2020"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

Name: Cherie Hua

Andrew ID: cxhua

This lab is to be begun in class, but may be finished outside of class at any time prior to Friday, October 16<sup>th</sup> at 6:00 PM. You must commit both the edited `Rmd` file and the "knitted" `html` file for this lab to your `GitHub` repo. (You can verbally discuss aspects of the labs with your classmates, but please do not share code, etc.)

---

# Preliminaries

## Goal

Today you will largely follow the script of Tuesday's lab, while applying random forest in place of classification and regression trees.

# Data, Part I

Below we read in the same data that we used on Monday, except that we downsample the data to have sizes 10000 (train) and 5000 (test) to keep computation time manageable. (Random forest takes time because you generate 500 trees by default.)
```{r}
rm(list=ls())
file.path = "https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/DM_GALAXY/Massive_Black_II.Rdata"
load(url(file.path))
rm(file.path)
objects()

resp.train = resp.train.df$prop.sfr
w = which(resp.train>0)
pred.train = pred.train[w,]
resp.train = log10(resp.train[w])

resp.test  = resp.test.df$prop.sfr
w = which(resp.test>0)
pred.test = pred.test[w,]
resp.test = log10(resp.test[w])

set.seed(202)
s = sample(length(resp.train),10000)
pred.train = pred.train[s,]
resp.train = resp.train[s]
s = sample(length(resp.test),5000)
pred.test = pred.test[s,]
resp.test = resp.test[s]

cat("Sample sizes: train = ",length(resp.train)," test = ",length(resp.test),"\n")
```

# Questions

## Question 1

Because we reduced the sample size relative to that in Tuesday's lab, re-determine the test-set MSE for a linear regression model and then determine the test-set MSE for a random forest model. (In the argument list for `randomForest`, set `importance=TRUE`) Also display the predicted response versus observed response diagnostic plot (with the same limits on each axis!). Did the test-set MSE improve by using random forest? (Note: for reproducible results, set the seed before running random forest!)
```{r}
#find mse
set.seed(14983)
lm.train = lm(resp.train ~ ., data = pred.train)
lm.preds = predict(lm.train, newdata = pred.test)
lm.mse = mean((resp.test - lm.preds)^2)
lm.mse

set.seed(95873)
library(randomForest)
library(GGally)
rf <- randomForest(formula = resp.train ~ ., data = pred.train, importance = TRUE)
yhat.rf = predict(rf, newdata = pred.test)
rf.mse = mean((yhat.rf - resp.test)^2)
rf.mse
ggplot(mapping = aes(yhat.rf, resp.test)) + geom_point(color = "green") + ylim(-3, 1) + xlim(-3, 1)
```
```
The random forest model slightly improved the MSE.
```

## Question 2

Create a variable importance plot for random forest. (See page 330 of ISLR.) You can subdivide the predictor variables into groups: those that reference dark matter particle velocity (ones with "v"), mass ("m"), or radius from the halo center ("r"), along with those that reference the gravitational potential (the ones with angle variables) and the shape of the halo ("shapes"). Do some inference: what is the most important property of a halo with regard to predicting star-formation rate? How about the least important?
```{r}
importance(rf)
```
```
The most important property seems to be velocity, and the least important seems to be shapes/radius.
```

Now we turn our attention to classification.

# Data, Part II

We will now load the second dataset from Monday, but we will do two things: (1) cut down the sample size to get random forest to run faster, and (2) balance the classes.
```{r}
rm(list=ls())
file.path = "https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/TD_CLASS/css_data.Rdata"
load(url(file.path))
rm(file.path)
objects()

# Eliminate the max.slope column (the 11th column), which has infinities.
predictors = predictors[,-11]

# Cut the CB and NON-CB class sizes to 5000 samples each.
set.seed(303)
w = which(response==1)
s = sample(length(w),5000)
predictors.cb = predictors[w[s],]
response.cb   = response[w[s]]
w = which(response!=1)
s = sample(length(w),5000)
predictors.noncb = predictors[w[s],]
response.noncb   = response[w[s]]
predictors = rbind(predictors.cb,predictors.noncb)
response   = c(response.cb,response.noncb)

response.new = rep("CB",length(response))
w = which(response!=1)
response.new[w] = "NON-CB"
response = factor(response.new)
```

# Questions

## Question 3

You know the drill: split the data (or do CV if you are feeling adventurous), then learn a logistic regression model and output the test-set MCR value and the confusion matrix. Then learn a random forest model, output the variable importance plot and the test-set MCR value and the confusion matrix. Compare and contrast the results: would you use the logistic regression model or the random forest model? As for variable importance: when using a tree, flux.mid35, mad, and skew tended to be important predictors. Is that result consistent with what you observe here? (Note: for logistic regression, <tt>predict()</tt> with <tt>type="response"</tt> gives you numbers you need to round off, whereas for random forest, you'll get actual CB vs. NON-CB predictions. Sigh. Inconsistency is not a virtue.)
```{r}
#sample
set.seed(45)
s = sample(nrow(predictors), 0.7*nrow(predictors))
pred.train = predictors[s,]
resp.train = response[s]

pred.test = predictors[-s,]
resp.test = response[-s]

#log reg
set.seed(385)
glm.train = glm(resp.train ~ ., data = pred.train, family = "binomial")
glm.preds = predict(glm.train, newdata = pred.test, type = "response")
table(ifelse(glm.preds < 0.5, "CB", "NON-CB"), resp.test)
(350 + 478)/(1164 + 1008)

#random forest
library(randomForest)
rf <- randomForest(formula = resp.train ~ ., data = pred.train, importance = TRUE)
yhat.rf = predict(rf, newdata = pred.test)
table(yhat.rf, resp.test)
#mcr
(230 + 359)/(1284 + 1127)
importance(rf)
```
```
I would use the random forest model. The important variables are amp, beyond.std, and flux.mid35, so it's not consistent with what usually happens.
```

## Question 4

Install and load the <tt>pROC</tt> package, and plot ROC curves for both logistic regression and random forest. (Google the documentation for the <tt>pROC</tt> package to see how to do this; basically, call the <tt>roc()</tt> function and pass its output to <tt>plot()</tt>. In order to put both curves on the same plot: call <tt>plot()</tt> once, then call <tt>plot()</tt> again with the argument <tt>add=TRUE</tt>. This adds a curve to an existing plot. Also, to tell the curves apart: make them different colors. If you want to be adventurous: add a <tt>legend()</tt>.) For logistic regression, use the class probabilities that you get by calling <tt>predict()</tt> with <tt>type="response"</tt>, whereas for random forest, call <tt>predict()</tt> with <tt>type="prob"</tt>, <i>then</i> extract the <i>second</i> column. Which model has the best AUC? (Try examining the output from <tt>roc()</tt> to determine this. Remember <tt>names()</tt>?)
```{r}
library(pROC)
glm.roc <- roc(resp.test, glm.preds)
rf.roc <- roc(resp.test, predict(rf, newdata = pred.test, type = "prob")[,2])
plot(glm.roc, col = "green")
plot(rf.roc, add = TRUE, col = "blue")
#green = linear regression
#blue = random forest
glm.roc
rf.roc
```
```
The random forest model has the better AUC.
```

## Question 5

There are many ways to determine what the optimum class-separation threshold would be for any given analysis. One way that is based directly on ROC curves is to determine which threshold maximizes Youden's J statistic: J = specificity + sensitivity - 1. (Basically, it gives the same cost to misspecification and to missensitivity...it is what you use if you seek accuracy in both classes simultaneously.) For random forest, determine which threshold value maximizes Youden's J statistic. (The object output by <tt>roc()</tt> contains all the information you need!) In retrospect, are you surprised by the value? Is specificity or sensitivity higher?
```{r}
coords(rf.roc, x = "best", input = "threshold", best.method = "youden", transpose = TRUE)
```
```
I don't think I'm surprised by this value. The specificity is higher.
```
